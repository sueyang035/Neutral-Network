{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LvBxvEhO18P"
      },
      "source": [
        "# Create a XOR logical operator using multilayer perceptrons. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KonyXCGPP0X-"
      },
      "source": [
        "### Functions\n",
        "write a step function and convert to vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwvI7xKFRDC5",
        "outputId": "2f116ccd-1a80-4a83-d62c-750feb0e0a3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "def step(x):\n",
        "    ### Code star here ###\n",
        "    if x>=0:\n",
        "        return(1)\n",
        "    else:\n",
        "        return (0)\n",
        "vstep = np.vectorize(step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2aIsGhhbKWk"
      },
      "source": [
        "### Class and Perceptron\n",
        "\n",
        "Recall the definition of a single perceptron:\n",
        "$$\n",
        "\\hat{y}=\\phi(w^{\\top}x + b)\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "RR7vYaelcAIq"
      },
      "outputs": [],
      "source": [
        "class perceptron:\n",
        "    def __init__(self, w, b):\n",
        "        self.w = np.array(w)\n",
        "        self.b = b\n",
        "\n",
        "    def __call__(self, x):\n",
        "        ### Code star here ###\n",
        "        return (vstep(np.dot(self.w,x) +self.b))\n",
        "        ### End code here ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvxynUz4cTrE"
      },
      "source": [
        "\n",
        "Define the AND, OR, and NAND gates using the `Perceptron` class:\n",
        "1. `and_perceptron`: `w=[1, 1]` and `b=-1.5`.\n",
        "2. `or_perceptron`: `w=[1, 1]` and `b=-0.5`.\n",
        "3. `nand_perceptron`: `w=[-1, -1]` and `b=1.5`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "zsO-HQfGcTCv"
      },
      "outputs": [],
      "source": [
        "and_perceptron = perceptron([1, 1], -1.5)\n",
        "or_perceptron = perceptron([1, 1], -0.5)\n",
        "nand_perceptron = perceptron([-1,-1],1.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa-gj_bod92e"
      },
      "source": [
        "### Multilayer Perceptrons\n",
        "\n",
        "As discussed in the first week's lecture, we can implement `xor` using a two-layer MLP. This is possible because `xor` can be expressed in terms of `or`, `and`, and `nand` as follows:\n",
        "$$\n",
        "x_1 \\oplus x_2 = (x_1 \\lor x_2) \\land (x_1 \\uparrow x_2).\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSAB2oued9TD",
        "outputId": "94dbdc3d-c94a-48c7-aa02-bfc341a4507d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: [0 0], XOR Output: 0\n",
            "Input: [0 1], XOR Output: 1\n",
            "Input: [1 0], XOR Output: 1\n",
            "Input: [1 1], XOR Output: 0\n"
          ]
        }
      ],
      "source": [
        "def xor_mlp(x):\n",
        "    # First layer: we compute the outputs from or_perceptron and nand_perceptron\n",
        "    out1 = or_perceptron(x)\n",
        "    ### Code star here ###\n",
        "    out2 = nand_perceptron(x)\n",
        "    ### End code here ###\n",
        "\n",
        "    # Second layer: we compute the final output from and_perceptron\n",
        "    final_output = and_perceptron([out1, out2])\n",
        "    return final_output\n",
        "# test the function\n",
        "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "for x in inputs:\n",
        "    print(f\"Input: {x}, XOR Output: {xor_mlp(x)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
