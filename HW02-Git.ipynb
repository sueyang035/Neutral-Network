{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr6Wlxltv6Z-"
   },
   "source": [
    "# Implement a two layer NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lVhxpgqHuD-i"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1) # set a seed so that the results are consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeN8KBzsJQ00"
   },
   "source": [
    "## 1- Definining the neural network structure\n",
    "\n",
    "In this exercise, you will implement a two-layer neural network, also known as a multilayer perceptron (MLP), with one hidden layer. Given a training sample $(x,y)$, the forward propagation of the network is defined as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z^1 =& W^1 x + b^1\\\\\n",
    "a^1 =& \\phi(z^1)\\\\\n",
    "z^2 =& W^2 a^1 + b^2\\\\\n",
    "a^2 =& \\phi(z^2)\n",
    "\\end{align*}\n",
    "$$\n",
    "where\n",
    "- $W^i$ are the weights\n",
    "- $b^i$ are the bias\n",
    "- $z^i$ are the pre-activaiton,\n",
    "- $a^i$ are the activaiton\n",
    "\n",
    "The network's output is $a^2$, that is then compare to the true label $y$ using the square loss function:\n",
    "$$\n",
    "\\ell(a,y) = \\frac{1}{2}(a-y)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KX4jyaK9LS07"
   },
   "source": [
    "**Exercise 1**:\n",
    "Define three values:\n",
    "- `n_x`: the size of the input data\n",
    "- `n_h`: the size of hidden layer, i.e., the number neurons in the hidden layer. The default value is $5$\n",
    "- `n_y`: the size of the output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pm3uIAqRImDa"
   },
   "outputs": [],
   "source": [
    "def neural_network_structure(X, Y, n_h=5):\n",
    "    n_x = X.shape[0]\n",
    "    ### Code star here ### (~ 1 lines of code)\n",
    "    n_y=Y.shape[0]\n",
    "\n",
    "    ### End code here ###\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3keagCFJMrI3",
    "outputId": "f9fbb555-39ae-4eda-c260-ebef3ba5dfd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the input data: n_x = 2\n",
      "The size of the hidden layer: n_h = 10\n",
      "The size of the output: n_y = 1\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "Y = np.random.randn(1, 3)\n",
    "n_x, n_h, n_y = neural_network_structure(X, Y, 10)\n",
    "\n",
    "print(\"The size of the input data: n_x = \" +str(n_x))\n",
    "print(\"The size of the hidden layer: n_h = \" +str(n_h))\n",
    "print(\"The size of the output: n_y = \" +str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ-hQEQHNZvm"
   },
   "source": [
    "## 2 - Random Initialization\n",
    "\n",
    "**Excecise 2**: implement the function `initialize_parameters()`.\n",
    "To avoid symmetric patterns in neural networks, we’ll use random initialization for the weights.\n",
    "\n",
    "1. The function `initialize_parameters()` has input `n_x`, `n_h`, `n_y` as inputs.\n",
    "2. Use random normal distribution: `stdv * np.random.randn(a.b) + mu`, where `mu=0.0` and `stdv=1/np.sqrt(n_x)` for `W_1` and `stdv=1/np.sqrt(n_h)` for `W_2`\n",
    "3. Initialize biases as zeros with the correct shape: `np.zeros((a,b))`\n",
    "4. Return `parameters` as a dictionary containing all weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NRHewuF9MzYT"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    W1 = np.random.randn(n_h, n_x) / np.sqrt(n_x)\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    ### Code star here ### (~ 1 lines of code)\n",
    "    W2 = np.random.randn(n_y,n_h)/np.sqrt(n_h)\n",
    "    ### End code here ###\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ss4CnKVeP7pW"
   },
   "source": [
    "## 3 - Sigmoid Function and Its Derivatives\n",
    "As discussed in the lectures, the step function is unsuitable for training MLPs because its derivative is zero almost everywhere. Instead, we’ll use the sigmoid function as the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAA8QsqdQe2t"
   },
   "source": [
    "**Exercise 3**:\n",
    "1. Implement sigmoid function `sigmoid()` as $\\sigma(x)=\\frac{1}{1+e^{-x}}$\n",
    "2. Implement its derivative `sigmoid_derivative()` as $\\sigma^{\\prime}(x) = \\sigma(x) \\cdot (1-\\sigma(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RCiC1PCKPg2M"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    ### Code star here ### (~ 1 lines of code)\n",
    "    return 1/(1+np.exp(-x))\n",
    "    ### End code here ###\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6ty0zEmSQ9y"
   },
   "source": [
    "## 4 - Forward Propogation\n",
    "\n",
    "In the lecture, we covered the forward propagation for a 2-layer MLP using vectorization:\n",
    "$$\n",
    "\\begin{align}\n",
    "Z^1 &= W^1 * X + b^1\\\\\n",
    "A^1 &= \\phi(Z^1)\\\\\n",
    "Z^2 &= W^2 * A^1 + b^2\\\\\n",
    "A^2 &= \\phi(Z^2)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "*Note*: NumPy’s broadcasting mechanism allows a bias vector `b` (shape `(n_h, 1)`) to be automatically added to each column of $W \\times A$ (shape `(n_h, m)`), where `m` is the number of training samples.\n",
    "\n",
    " **Exercise 4**: Implementing forward propagation `forward_propagation()`\n",
    "\n",
    " 1. The function `forward_propagation()` takes `X` and `parameters` as inputs\n",
    " 2. Retrieve the weights and bias from `parameters`\n",
    " 3. Compute `Z1`, `A1`, `Z2`, and `A2` using the equations above.\n",
    " 4. Store intermediate variables in `cache` for use in backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "D8_VUXS3R4x6"
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    ### Code star here ### (~ 2 lines of code)\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    ### End code here ###\n",
    "\n",
    "    # Implement Forward Propagation to calculate A2\n",
    "    Z1 = W1 @ X + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    ### Code star here ### (~ 2 lines of code)\n",
    "    Z2 = W2 @ A1 + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    ### End code here ###\n",
    "\n",
    "    # Store the intermedaite valeus in \"cache\" for backpropagation\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    return A2, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.19151237249896635 0.4688525159515502 -0.3118444809339782 0.42266909957970195\n"
     ]
    }
   ],
   "source": [
    "A2, cache = forward_propagation(X, parameters)\n",
    "\n",
    "print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3De3ZhSWHBj"
   },
   "source": [
    "## 5 - Compute the Cost\n",
    "With the output estimate `A2` from forward propagation, we compute the cost using the square loss:\n",
    "$$\n",
    "L(\\theta)=\\frac{1}{2m} \\sum_{i=1}^{m} (a_i-y_i)^2\n",
    "$$\n",
    "\n",
    "**Exercise 5**: Implement `compute_cost()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VCbyvFrbVxCQ"
   },
   "outputs": [],
   "source": [
    "def computer_cost(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    ### Code star here ### (~ 1 lines of code)\n",
    "    cost = np.sum((A2-Y)**2)/(2*m)\n",
    "    ### End code here ###\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EuKAsSC2Wrz7",
    "outputId": "de8d1889-9d30-47a5-8ef6-641f14bdd727",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.5239053069310721\n"
     ]
    }
   ],
   "source": [
    "print(f\"cost = {computer_cost(A2, Y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64bUYo3BW9Hm"
   },
   "source": [
    "## 6 - Backpropagation\n",
    "Using the `cache` computed during the forward propogation, we can compute the gradients through backpropogation\n",
    "$$\n",
    "\\begin{align}\n",
    "&d Z^2 = \\frac{1}{m}( A^{2} - Y) \\odot \\phi^{\\prime}( Z^{2}) \\\\\n",
    "&d W^2 = d Z^2 (A^1)^{\\top}\\\\\n",
    "&d b^2 = \\sum_{i=1}^{m} dZ^2_i\\\\\n",
    "&d Z^1 = ((W^2)^{\\top} dZ^2 ) \\odot \\phi^{\\prime}(Z^1)\\\\\n",
    "&d W^1 = d Z^1 X^{\\top}\\\\\n",
    "&d b^1 = \\sum_{i=1}^{m} dZ^1_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Exercise 6:** Implement `back_propogation()`\n",
    "1. The function `back_propogation()` takes data `X` and `Y`, weights and biases in `parameters`, and `cache` as inputs\n",
    "2. Retrive weights (`W1` and `W2`) and biase (`b1` and `b2`) from `parameters`\n",
    "3. Retrive cached variables (`Z1`, `Z2`, `A1`, and `A2`) from `cache`\n",
    "4. Compute the gradients `dW1`, `dW2`, `db1`, `db2` using the provided formulas and you may also need to compute `dZ2` and `dZ1` as needed\n",
    "5. Return gradients in a variable `grads`\n",
    "\n",
    "*Note*: when implement `db2` or `db2`, you may consider use `np.sum()`. Let `M` be a matrix with shape `(a,b)`. Then `np.sum(M, axis=0)` sums each column, while `np.sum(M, axis=1)` sums each row. Use `keepdims=True` to maintain the dimensions summing. For example, `np.sum(M, axis=1, keepdims=True)` is used to sum across rows while preserving the shape needed for broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-2FEJ7VkWzbR"
   },
   "outputs": [],
   "source": [
    "def b1ack_propogation(X, Y, parameters, cache):\n",
    "  # Retrieve each parameter from the dictionary \"parameters\"\n",
    "  W1 = parameters[\"W1\"]\n",
    "  b1 = parameters[\"b1\"]\n",
    "  W2 = parameters[\"W2\"]\n",
    "  b2 = parameters[\"b2\"]\n",
    "\n",
    "  # Retrieve each value from the dictionary \"cache\"\n",
    "  Z1 = cache[\"Z1\"]\n",
    "  A1 = cache[\"A1\"]\n",
    "  Z2 = cache[\"Z2\"]\n",
    "  A2 = cache[\"A2\"]\n",
    "\n",
    "  # Compute gradients: dW1, db1, dW2, db2\n",
    "  m = Y.shape[1]\n",
    "  dZ2 = (A2 - Y)/m * sigmoid_derivative(Z2)\n",
    "  dW2 = dZ2 @ A1.T\n",
    "  db2 = np.sum(dZ2, axis=1, keepdims=True)\n",
    "  ### Code star here ### (~ 3 lines of code)\n",
    "  dZ1 = W2.T*dZ2*sigmoid_derivative(Z1)\n",
    "  dW1 = dZ1 @ X.T\n",
    "  db1 = np.sum(dZ1,axis=1,keepdims=True)\n",
    "  ### Code end here ###\n",
    "\n",
    "  # Stores the gradients\n",
    "  grads = {\"dW1\": dW1,\n",
    "           \"db1\": db1,\n",
    "           \"dW2\": dW2,\n",
    "           \"db2\": db2}\n",
    "  return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[-0.00664941  0.00551946]\n",
      " [ 0.00666033 -0.00568816]\n",
      " [ 0.00529232 -0.00427319]\n",
      " [ 0.01028577 -0.00799248]\n",
      " [ 0.01305662 -0.0099341 ]\n",
      " [ 0.008174   -0.0066037 ]\n",
      " [ 0.00021912 -0.00017832]\n",
      " [ 0.02062341 -0.01630933]\n",
      " [-0.004029    0.00306173]\n",
      " [-0.02714012  0.02102039]]\n",
      "db1 = [[ 4.00223279e-04]\n",
      " [-2.71719312e-03]\n",
      " [-3.78546949e-04]\n",
      " [-1.13124221e-03]\n",
      " [-1.15711387e-03]\n",
      " [-9.91044386e-04]\n",
      " [-1.93340979e-06]\n",
      " [ 8.46922963e-04]\n",
      " [ 1.30854178e-04]\n",
      " [ 1.60536914e-03]]\n",
      "dW2 = [[ 0.04935606  0.05904166  0.04547199  0.0361181   0.03417343 -0.05364016\n",
      "  -0.01991294  0.00616847  0.02248469  0.02777064]]\n",
      "db2 = [[-0.0032622]]\n"
     ]
    }
   ],
   "source": [
    "grads = b1ack_propogation(X, Y, parameters, cache)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dW2 = \"+ str(grads[\"dW2\"]))\n",
    "print (\"db2 = \"+ str(grads[\"db2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa8E6b1OdvLO"
   },
   "source": [
    "## 7 - Update Weights and Biases Using Gradient Descent\n",
    "Gradient descents are performed using:\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta d\\theta\n",
    "$$\n",
    "where $\\eta>0$ is the learning rate.\n",
    "\n",
    "**Exercise 7**: Implemente `update_parameters()`\n",
    "1. The function takes `parameters`, `grads`, and `learning_rate` as inputs\n",
    "2. Retrive weights and biases from `parameters`\n",
    "3. Retrive gradients from `grads`\n",
    "4. Update the weights and biases using the gradient descent rule.\n",
    "5. Store the updated weights and biases back into `parameters` and return them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "B9GLhBWVXYLi"
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "  # Retrieve each parameter from the dictionary \"parameters\"\n",
    "  W1 = parameters[\"W1\"]\n",
    "  b1 = parameters[\"b1\"]\n",
    "  ### Code star here ### (~ 2 lines of code)\n",
    "  W2 = parameters[\"W2\"]\n",
    "  b2 = parameters[\"b2\"]\n",
    "  ### End code here ###\n",
    "\n",
    "  # Retrive gradients from the dictionary \"grads\"\n",
    "  dW1 = grads[\"dW1\"]\n",
    "  db1 = grads[\"db1\"]\n",
    "  ### Code star here ### (~ 2 lines of code)\n",
    "  dW2 =grads[\"dW2\"]\n",
    "  db2 =grads[\"db2\"]\n",
    "  ### End code here ###\n",
    "\n",
    "  # Update weights and biases using gradient descent update rule\n",
    "  W1 = W1 - learning_rate * dW1\n",
    "  b1 = b1 - learning_rate * db1\n",
    "  ### Code star here ### (~ 2 lines of code)\n",
    "  W2 = W2 - learning_rate * dW2\n",
    "  b2 = b2 - learning_rate * db2\n",
    "  ### End code here ###\n",
    "\n",
    "  # Store the updated weights and biases back into \"parameters\"\n",
    "  parameters = {\"W1\": W1,\n",
    "                \"b1\": b1,\n",
    "                \"W2\": W2,\n",
    "                \"b2\": b2}\n",
    "  return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.17626499  1.03381124]\n",
      " [-1.45680607 -0.22792651]\n",
      " [-0.27162036  0.80173879]\n",
      " [-0.77784343 -0.12184523]\n",
      " [-0.62087021  0.02994897]\n",
      " [ 0.41203085 -0.77818925]\n",
      " [ 0.80943971  0.6375227 ]\n",
      " [ 0.35511092  0.63716444]\n",
      " [-0.48342832 -0.08692713]\n",
      " [-0.66141751 -0.18963568]]\n",
      "b1 = [[-4.00223279e-06]\n",
      " [ 2.71719312e-05]\n",
      " [ 3.78546949e-06]\n",
      " [ 1.13124221e-05]\n",
      " [ 1.15711387e-05]\n",
      " [ 9.91044386e-06]\n",
      " [ 1.93340979e-08]\n",
      " [-8.46922963e-06]\n",
      " [-1.30854178e-06]\n",
      " [-1.60536914e-05]]\n",
      "W2 = [[ 0.16721956 -0.21931275 -0.1259192  -0.21766427 -0.26761923 -0.21173026\n",
      "  -0.00380577 -0.35338624  0.07390391  0.52459783]]\n",
      "b2 = [[3.262196e-05]]\n"
     ]
    }
   ],
   "source": [
    "parameters = update_parameters(parameters, grads, 0.01)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gb8WlNSfSD-"
   },
   "source": [
    "## 8 - Training Loop\n",
    "\n",
    "**Exercise 8**: Integrate the preivous parts into a function `train_loop()`.\n",
    "1. The function `train_loop()` takes input data `(X,Y)` and network size `n_h`, and `learning_rate` with `max_iteration` as inputs\n",
    "2. Retrive `(n_x,n_h,n_y)` using `neural_network_structure()`\n",
    "3. Initialize the parameters using `initialize_parameters()`\n",
    "4. Create a `for` loop to train the network by calling `forward_propagation()` to compute the `cost` and `cach`, `back_propagation()` to compute the `grads`, then `update_parameters()` to update `parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "n3rHawCvXYQB"
   },
   "outputs": [],
   "source": [
    "def train_loop(X, Y, n_h, learning_rate, max_iteration, print_cost=True):\n",
    "  # Retrive (n_x, n_h, n_y)\n",
    "  n_x, n_h, n_y = neural_network_structure(X, Y, n_h)\n",
    "\n",
    "  # Initialize the parameters\n",
    "  parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "  for iter in range(max_iteration):\n",
    "    # Forward propagation\n",
    "    ### Code star here ### (~ 1 lines of code)\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    ### End code here ###\n",
    "\n",
    "    # Compute loss\n",
    "    cost = computer_cost(A2, Y)\n",
    "    if print_cost:\n",
    "      print(f\"Epoch {iter}: Loss = {cost}\")\n",
    "\n",
    "    # Backward propagation\n",
    "    ### Code star here ### (~ 1 lines of code)\n",
    "    grads = b1ack_propogation(X, Y, parameters, cache)\n",
    "    ### End code here ###\n",
    "    \n",
    "    # Update parameters\n",
    "    ### Code star here ### (~ 1 lines of code)\n",
    "    parameters = update_parameters(parameters, grads, learning_rate)\n",
    "    ### End code here ###\n",
    "\n",
    "  return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.5413416581298806\n",
      "Epoch 1: Loss = 0.5411639794561593\n",
      "Epoch 2: Loss = 0.5409863314471564\n",
      "Epoch 3: Loss = 0.540808715111546\n",
      "Epoch 4: Loss = 0.5406311314554088\n",
      "Epoch 5: Loss = 0.5404535814821864\n",
      "Epoch 6: Loss = 0.5402760661926396\n",
      "Epoch 7: Loss = 0.5400985865848044\n",
      "Epoch 8: Loss = 0.5399211436539499\n",
      "Epoch 9: Loss = 0.5397437383925353\n"
     ]
    }
   ],
   "source": [
    "parameters = train_loop(X, Y, 10, 0.01, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1M437dYXiHqo"
   },
   "source": [
    "## 8.1 - Training on Real Dataset\n",
    "\n",
    "Let us test your MLP model on the MNIST dataset, which contains images of digits `0` through `9`. For simplicity, we will select only the digits `0` and `1` for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BhQuZPfdiGKd",
    "outputId": "26a8ba33-a770-4d3d-89ed-51265bb1b74d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 17:55:03.594619: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Training data shape: (784, 12665)\n",
      "Training labels shape: (1, 12665)\n",
      "Testing data shape: (784, 2115)\n",
      "Testing labels shape: (1, 2115)\n",
      "Training labels: [0 1]\n",
      "Testing labels: [0 1]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Flatten the 28x28 images into vectors of 784 elements and normalize to [0, 1]\n",
    "X_train = X_train.reshape(X_train.shape[0], -1).T / 255.0  # Transpose to (in_features, num_samples)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).T / 255.0     # Transpose to (in_features, num_samples)\n",
    "\n",
    "# Select only the samples of class '0' and '1' for binary classification\n",
    "train_filter = (y_train == 0) | (y_train == 1)\n",
    "test_filter = (y_test == 0) | (y_test == 1)\n",
    "\n",
    "X_train_binary = X_train[:, train_filter]\n",
    "y_train_binary = y_train[train_filter].reshape(1, -1)  # Reshape to (1, num_samples)\n",
    "\n",
    "X_test_binary = X_test[:, test_filter]\n",
    "y_test_binary = y_test[test_filter].reshape(1, -1)  # Reshape to (1, num_samples)\n",
    "\n",
    "# Verify the shapes\n",
    "print(f\"Training data shape: {X_train_binary.shape}\")  # Should be (784, num_samples)\n",
    "print(f\"Training labels shape: {y_train_binary.shape}\")  # Should be (1, num_samples)\n",
    "print(f\"Testing data shape: {X_test_binary.shape}\")  # Should be (784, num_samples)\n",
    "print(f\"Testing labels shape: {y_test_binary.shape}\")  # Should be (1, num_samples)\n",
    "\n",
    "# Print out some example labels to verify\n",
    "print(\"Training labels:\", np.unique(y_train_binary))\n",
    "print(\"Testing labels:\", np.unique(y_test_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.13540832825476667\n",
      "Epoch 1: Loss = 0.13526546010137525\n",
      "Epoch 2: Loss = 0.13512275737068838\n",
      "Epoch 3: Loss = 0.13498022020623673\n",
      "Epoch 4: Loss = 0.13483784874801413\n",
      "Epoch 5: Loss = 0.13469564313249144\n",
      "Epoch 6: Loss = 0.13455360349263087\n",
      "Epoch 7: Loss = 0.13441172995790018\n",
      "Epoch 8: Loss = 0.13427002265428714\n",
      "Epoch 9: Loss = 0.13412848170431413\n"
     ]
    }
   ],
   "source": [
    "parameters = train_loop(X_train_binary, y_train_binary, 10, 0.01, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfb-_P45m4K2"
   },
   "source": [
    "### 9 - Predictions\n",
    "Use your trained model to make predictions by building `predict()`.\n",
    "\n",
    "**Exercise 9:**\n",
    "1. The function `predict()` takes `X` and `parameters` as inputs\n",
    "2. Call `forward_propagation()` to obtain the output `A2`\n",
    "3. Assign labels using threshold `0.5`; label class `1` if `A2 > 0.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "QcAuTkBghvST"
   },
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    ### Code star here ### (~ 1 lines of code)\n",
    "    predictions = np.where(A2 > 0.5, 1, 0)\n",
    "    ### End code here ###\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.46766679826292934\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(X_train_binary, parameters)\n",
    "print(f\"Training Accuracy: {np.mean(predictions == y_train_binary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhhGfU4iohkZ",
    "outputId": "70a05f79-5093-4c71-c720-5268472cd0d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.46335697399527187\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(X_test_binary, parameters)\n",
    "print(f\"Testing Accuracy: {np.mean(predictions == y_test_binary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhpyVaKLow0V"
   },
   "source": [
    "## 10 - Tuning Network Hyperparameters\n",
    "\n",
    "In this two-layer MLP, we have the following hyperparameters: network size `n_h`, `learning_rate`, and `max_iteration`. The choice of these values will influence the network's performance.\n",
    "\n",
    "**Exercise 10**: Experiment with different values for network size `n_h` to observe how the network size influences performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aUlGtGoVp84g",
    "outputId": "cf4b0cbb-2d41-4881-d2dd-74e7878c7de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Size: 5, Training Accuracy: 0.46766679826292934, Testing Accuracy: 0.46335697399527187\n",
      "Network Size: 10, Training Accuracy: 0.49206474536123174, Testing Accuracy: 0.4907801418439716\n",
      "Network Size: 20, Training Accuracy: 0.9760757994472957, Testing Accuracy: 0.9853427895981087\n",
      "Network Size: 30, Training Accuracy: 0.5769443347808922, Testing Accuracy: 0.5947990543735224\n",
      "Network Size: 40, Training Accuracy: 0.4839320963284643, Testing Accuracy: 0.48226950354609927\n",
      "Network Size: 50, Training Accuracy: 0.931227793130675, Testing Accuracy: 0.932387706855792\n",
      "Network Size: 60, Training Accuracy: 0.5336754836162653, Testing Accuracy: 0.5371158392434988\n",
      "Network Size: 70, Training Accuracy: 0.5344650611922621, Testing Accuracy: 0.5399527186761229\n",
      "Network Size: 80, Training Accuracy: 0.7106987761547572, Testing Accuracy: 0.7087470449172577\n",
      "Network Size: 90, Training Accuracy: 0.9602052901697592, Testing Accuracy: 0.9621749408983451\n",
      "Network Size: 100, Training Accuracy: 0.7722858270825108, Testing Accuracy: 0.7782505910165485\n"
     ]
    }
   ],
   "source": [
    "network_sizes = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "for n_h in network_sizes:\n",
    "    parameters = train_loop(X_train_binary, y_train_binary, n_h, 0.1, 10, False)\n",
    "    ### Code star here ### (~ 4 lines of code)\n",
    "    Train_predictions = predict(X_train_binary, parameters)\n",
    "    train_accuracy = np.mean(Train_predictions == y_train_binary)\n",
    "    Test_predictions = predict(X_test_binary, parameters)\n",
    "    test_accuracy = np.mean(Test_predictions==y_test_binary)\n",
    "    ### End code here ###\n",
    "    print(f\"Network Size: {n_h}, Training Accuracy: {train_accuracy}, Testing Accuracy: {test_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
